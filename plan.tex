Robots operating in unstructured, stochastic environments such as a
factory floor or a kitchen face a difficult planning problem due to
the large state space and the very large set of possible
tasks~\citep{bollini12,knepper13}.  A powerful and flexible robot such
as a mobile manipulator in the home has a very large set of possible
actions, any of which may be relevant depending on the current goal
(for example, robots assembling furinture~\citep{knepper13} or baking
cookies~\citep{bollini12}.)  When a robot is manipulating objects in
an environment, an object can be placed anywhere in a large set of
locations.  The size of the state space increases exponentially with
the number of objects, which bounds the placement problems that the
robot is able to expediently solve.  Depending on the reward function
(which is unknown before runtime), any of these states and actions may
be relevant to the solution, but for any specific reward function,
most of them are irrelevant.  For instance, when making brownies, the
oven and flour are important, while the soy sauce and saut\'{e} pan
are not.  For a different task, such as stir-frying broccoli, the
robot must use a different set of objects and
actions. 
\begin{figure}
\centering
\includegraphics[width=0.25\linewidth]{figures/smelt_small.jpg}
\includegraphics[width=0.25\linewidth]{figures/smelt_large.jpg}
\caption{Two different problems from the same domain, where the
  agent's goal is to smelt the gold in the furnace while avoiding the
  lava.  Our agent is unable to solve the problem on the right before
  learning because the state/action space is too large (since it can
  place the gold block anywhere).  After learning, it can quickly
  solve the larger problem.\label{fig:example}}
\end{figure}

To confront this state-action space explosion, prior work has explored
adding knowledge to the planner, such as options~\cite{sutton99} and
macro-actions~\cite{Botea:2005kx,Newton:2005vn}.  However, while these
methods can allow the agent to search more deeply in the state space,
they add non-primitive actions to the planner which {\em increase} the
branching factor of the state-action space.  The resulting augmented
space is even larger, which can have the paradoxical effect of
increasing the search time for a good policy~\cite{Jong:2008zr}.
Deterministic forward-search algorithms like hierarchical task
networks (HTNs)~\citep{Nau:1999:SSH:1624312.1624357}, and temporal
logical planning
(TLPlan)~\citep{Bacchus95usingtemporal,Bacchus99usingtemporal}, add
knowledge to the planner that greatly increases planning speed, but do
not generalize to stochastic domains. Additionally, the knowledge
provided to the planner by these methods is quite extensive, reducing
the agent's autonomy and must be manually supplied by the designer.

To address these issues, we augment an Object Oriented Markov Decision
Process (OO-MDP) with a specific type of action prior conditioned on
the current state and an abstract goal description.  Because we
condition on both the state and goal description, we refer to this
goal-based action prior as a knowlege base of {\em affordances}.
Affordances were originally proposed by \citet{gibson77} as action
possibilities prescribed by an agent's capabilities in an environment.
%Affordances focus an agent's attention on aspects of the environment that
%are most relevant to solving its current goal and avoid exploration of irrelevant parts of the
%world.
We rigorously formalize the notion of affordances as a prior on
actions conditioned on features of the current state as well as the
robot's goal.  Affordances enable the robot to prune irrelevant
actions on a state-by-state basis based on the agent's current goal
and focus on the most promising parts of the state space.  Affordances
can be specified by hand or alternatively learned through experience
in related problems, making them a concise, transferable, and
learnable means of representing useful planning knowledge. Our
experiments demonstrate that affordances provide dramatic improvements
for a variety of planning tasks compared to baselines in simulation,
and are applicable across different state spaces.  Moreover, while
manually provided affordances outperform baselines, affordances
learned through experience yield even greater improvements.We conduct
experiments in the game Minecraft, which has a very large state-action
space, and on a real-world robotic cooking assistant.
Figure~\ref{fig:example} shows an example of two problems from the
same domain in the game Minecraft; the agent learns on randomly
generated problems and tests on new problems from the same domain that
it has never previously encountered. 


 To learn affordances,
we provide a set of training worlds from the domain ($W$), for which
the optimal policy, $\pi$, may be tractably computed using existing
planning methods. Then, we compute the maximum likelihood estimate of the
parameter vector $\theta_i$ for each action using the policy.

During the learning phase, the agent learns which actions are useful
under different conditions. At test time, the agent will see
different, randomly generated worlds from the same domain, and use the
learned affordances to increase its speed at inferring a plan.  For
simplicity, our learning process uses a strict separation between
training and test; after learning is complete our model parameters
remain fixed.

We evaluate our approach using the game Minecraft.  Minecraft is a 3-D
blocks game in which the user can place, craft, and destroy blocks of
different types.  Minecraft's physics and action space allow users to
create complex systems, including logic gates and functional
scientific graphing
calculators\footnote{https://www.youtube.com/watch?v=wgJfVRhotlQ}.
Minecraft serves as a model for robotic tasks such as cooking
assistance, assembling items in a factory, object retrieval, and
complex terrain traversal.

Our experiments consisted of five common tasks in Minecraft, including
constructing bridges over trenches, smelting gold, tunneling through
walls, basic path planning, and digging to find an object.  We tested
on randomized worlds of varying size and difficulty. The generated
test worlds varied in size from tens of thousands of states to
hundreds of thousands of states.  The agent learned affordances from a
training set consisting of 25 simple state spaces of each map type
(100 total maps), each approximately a 1,000-10,000 state world. We
conducted all tests with a single knowledge base. Learning this knowledge base
took approximately one hour run in parallel on a computing grid. 


We use Real-Time Dynamic Programming (RTDP)~\cite{barto95} as our
baseline planner, a sampling-based algorithm that does not require the
planner to visit all states. We compare RTDP with learned
affordance-aware RTDP (LA-RTDP), and expert-defined affordance-aware
RTDP (EA-RTDP). We terminated each planner when the maximum change in
the value function was less than 0.01 for 100 consecutive policy
rollouts, or the planner failed to converge after 1000 rollouts.  The
reward function was $-1$ for all transitions, except transitions to
states in which the agent was in lava, where we set the reward to
$-10$. The goal was set to be terminal and the discount factor was
$\gamma = 0.99$.  To introduce non-determinism into our problem,
movement actions (move, rotate, jump) in all experiments had a small
probability (0.05) of incorrectly applying a different movement
action.  This noise factor approximates noise faced by a physical
robot that attempts to execute actions in a real-world domain and
can affect the optimal policy due to the existence of lava pits
that the agent can fall into. 


% -- Figure: Average results --
\begin{figure}[t]
%\begin{figure}
\centering
\includegraphics[width=0.3\linewidth]{figures/average_results_cropped.png}%
%\includegraphics[scale=0.18]{figures/average_results_cropped.png}%
\caption{Average results from all maps.}
\label{fig:average_results}
\end{figure}
%\end{figure}

