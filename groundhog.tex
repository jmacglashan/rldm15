\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}

\title{Escaping Groundhog Day}


\author{}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
Existing approaches to reinforcement learning rely on a fixed state-action space and reward function that the agent is trying to maximize. Often during training, the agent is reset to a predefined initial state or set of initial states. For example, in the classic RL Mountain Car domain \cite{}, the agent starts at some point in the valley, continues until it reaches the top of the valley and then resets to somewhere else in the same valley. Learning in this domain is akin to the learning problem faced by Bill Murray in the 1993 movie {\em Groundhog Day} in which he repeatedly relives the same day, until he discovers the optimal policy and escapes to the next day. Instead, a more natural formulation for many problems is that the agent samples an MDP from some unknown distribution of related MDPs and must learn to behave well under the distribution of MDPs. In our analogy every day is a new day that may have similarities to the previous day but the agent never encounters the same state twice. Given a new MDP drawn from the same distribution, we would like the agent to learn to solve it as quickly as possible. This formulation is a natural fit for robotics problems in which a robot is placed in a room in which it has never previously been, but has seen similar rooms with similar objects in the past. 

We formalize this problem as learning to act in a {\em domain}. A domain is defined by the tuple $(X, A, P)$, where $X$ is a state representation, $A$ is an action set, and $P$ is a probability distribution of MDPs with different state spaces, reward functions, and transition dynamics, but the same state representation $X$ and action set $A$. In principle, $P$ could have such a large variation that nothing learned in one MDP drawn from it is useful for another; however, we are interested in domains in which there strong commonalities. For example, in the real world, no two rooms may be the same, but physical and social constraints entail similarity in the mechanics of objects such as light switches and door knobs; we would like are agent to learn in this types of scenarios.

By formalizing the problem in this way, we are able to focus the learning problem to generalizing its knowledge to states it's never previously seen rather than overfitting to a single set of states and reward function. Focusing on generalization suggests a set of evaluation metrics in which at training time the agent learns from i.i.d. samples from the MDP distribution and at test time is evaluated on a new set of i.i.d. MDP samples drawn from the same distribution. In this setting there are two key problems: (1) learning to plan and (2) learning to learn. In learning to plan, the agent always knows the transition dynamics for each MDP; however, the agent can exploit its knowledge from solving previous MDPs from the distribution to more efficiently find good solutions in new MDPs. In learning to learn, the agent does not know the transition dynamics for each MDP, but can use knowledge from learning in previous MDPs to learn a portable model or bias its exploration through the state space. In this work, we formally describe this paradigm, describe how existing research fits into it, and present.






%In the 1993 movie {\em Groundhog Day}, Bill Murray plays a character, Phil, who suddenly finds himself forced to repeatedly relive the same day. Regardless of whether he stays up all night, or even dies, by what would be the next day's start, he is back in bed at the beginning of the day he just lived. By repeating the same day, Phil is able to learn how to accomplish amazing feats in that day that no other human would otherwise be able to accomplish without similarly being able to re-experience the same events. Groundhog Day is interesting precisely because it places the protagonist in such a recognizably unrealistic situation, yet AI research in reinforcement learning (RL) is typically more similar to Groundhog Day than it is to learning how to behave in real life. In RL, agents are dropped into a state of an environment, allowed to act in it for some length of time or until a terminal state is reached, and then are reset back to the beginning of the same environment again.

%Although studying RL in the context of a single environment has been an excellent starting point for research with a number of applications, it has a number of limitations that prevent algorithms from scaling to many real world problems. For example, suppose we wish to develop an assistant robot that can be deployed to any number of homes. Tuning a learning algorithm to behave in a well defined home environment is likely to lead to both forms of overfitting and underfitting. Overfitting can occur because we do not expect every home to be the same nor reset itself each day. Underfitting can occur if the agent does not generalize experience in one environment to new environments. In response to these limitations, we advocate for a problem formulation in which the agent must learn how to behave well in a distribution of environments from a sequence of environments drawn from the same distribution. We review existing work that can be adapted to this paradigm and highlight challenges to be addressed.

\end{abstract}

\keywords{
Meta-learning, transfer learning, learning to plan,
}



\startmain % to start the main 1-4 pages of the submission.

\section{Introduction}

\begin{itemize}
\item abstract rephrasing
\item 5-state chain Meta-RL illustrative example
\item summary of what we will review
\end{itemize}

\section{Problem Definition}
\begin{itemize}
\item overall problem definition
\item RL instantiation (true transition dynamics unknown)
\item planning instantiation (true transition dynamics provided)
\item evaluation metrics 
\end{itemize}

\section{Possible Directions}
\subsection{Meta-RL}
\subsection{Bayesian RL}
\subsection{Transfer Learning}
\subsection{Skill learning}
\subsection{Action Priors}

\section{Conclusion}

\end{document}