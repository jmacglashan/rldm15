\documentclass[11pt]{article} % For LaTeX2e
\usepackage{rldmsubmit,palatino}
\usepackage{graphicx}

\title{Escaping Groundhog Day}


\author{}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\maketitle

\begin{abstract}
\input{abstract.tex}

%% In this setting there are two key problems: (1) learning
%% to plan and (2) learning to learn. In learning to plan, the agent
%% always knows the transition dynamics for each MDP; however, the agent
%% can exploit its knowledge from solving previous MDPs from the
%% distribution to more efficiently find good solutions in new MDPs. In
%% learning to learn, the agent does not know the transition dynamics for
%% each MDP, but can use knowledge from learning in previous MDPs to
%% learn a portable model or bias its exploration through the state
%% space. 

%% In principle, $P$ could have such a large variation
%% that nothing learned in one MDP drawn from it is useful for another;
%% however, we are interested in domains in which there strong
%% commonalities. For example, in the real world, no two rooms may be the
%% same, but physical and social constraints entail similarity in the
%% mechanics of objects such as light switches and door knobs; we would
%% like are agent to learn in this types of scenarios.

%In the 1993 movie {\em Groundhog Day}, Bill Murray plays a character, Phil, who suddenly finds himself forced to repeatedly relive the same day. Regardless of whether he stays up all night, or even dies, by what would be the next day's start, he is back in bed at the beginning of the day he just lived. By repeating the same day, Phil is able to learn how to accomplish amazing feats in that day that no other human would otherwise be able to accomplish without similarly being able to re-experience the same events. Groundhog Day is interesting precisely because it places the protagonist in such a recognizably unrealistic situation, yet AI research in reinforcement learning (RL) is typically more similar to Groundhog Day than it is to learning how to behave in real life. In RL, agents are dropped into a state of an environment, allowed to act in it for some length of time or until a terminal state is reached, and then are reset back to the beginning of the same environment again.

%Although studying RL in the context of a single environment has been an excellent starting point for research with a number of applications, it has a number of limitations that prevent algorithms from scaling to many real world problems. For example, suppose we wish to develop an assistant robot that can be deployed to any number of homes. Tuning a learning algorithm to behave in a well defined home environment is likely to lead to both forms of overfitting and underfitting. Overfitting can occur because we do not expect every home to be the same nor reset itself each day. Underfitting can occur if the agent does not generalize experience in one environment to new environments. In response to these limitations, we advocate for a problem formulation in which the agent must learn how to behave well in a distribution of environments from a sequence of environments drawn from the same distribution. We review existing work that can be adapted to this paradigm and highlight challenges to be addressed.

\end{abstract}

\keywords{
Meta-learning, transfer learning, learning to plan,
}



\startmain % to start the main 1-4 pages of the submission.

\input{extended}

\end{document}
