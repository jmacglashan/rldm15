\section{Introduction}
Existing approaches to reinforcement learning rely on a fixed state-action space and reward function that the agent is trying to maximize. Often during training, the agent is reset to a predefined initial state or set of initial states. For example, in the classic RL Mountain Car problem~\cite{singh1996reinforcement}, the agent is tasked with driving an underpowered car out of a valley and the agent learns to do so over a series of episodes. At the start of an episode, the agent is placed in some random location in the valley and end when the agent drives out of the valley. After the episode ends a new episode begins and learning continues. Learning in this problem, and many other RL problems, is akin to the learning problem faced by Bill Murray in the 1993 movie {\em Groundhog Day} in which he repeatedly---and unrealistically---relives the same day, until he discovers the optimal policy and escapes to the next day. Instead, a more natural formulation for many problems is that the agent samples an MDP from some unknown distribution of related MDPs and must learn to behave well under the distribution of MDPs. In our analogy, the agent iteratively experiences new days that may have similarities to the previous days, but require acting in states it has not previously observed. Given a new MDP drawn from the same distribution, we would like the agent to learn to solve it as quickly as possible. This formulation is a natural fit for robotics problems in which a robot is placed in a room it has never previously seen, but has seen similar rooms with similar objects in the past. 

We formalize this problem as learning to act in a {\em domain}. A domain is defined by the tuple $(X, A, P)$, where $X$ is a state representation, $A$ is an action set, and $P$ is a probability distribution of MDPs with different state spaces, reward functions, and transition dynamics, but the same state representation $X$ and action set $A$. In principle, $P$ could have such a large variation that nothing learned in one MDP drawn from it is useful for another; however, we are interested in domains in which there strong commonalities. For example, in the real world, no two rooms may be the same, but physical and social constraints entail similarity in the mechanics of objects, such as light switches and door knobs; we would like are agent to learn in this type of scenario. 

There are two types of learning problems that can be addressed in this setting: (1) learning to plan and (2) learning to learn. In learning to plan, the agent always knows the transition dynamics for each MDP; however, the agent can exploit its knowledge from solving previous MDPs from the distribution to more efficiently find good solutions in new MDPs. In learning to learn, the agent does not know the transition dynamics for each MDP, but can use knowledge from learning in previous MDPs to learn a portable model or bias its exploration through the state space.

By formalizing the learning problem in this way, we are able to focus research on generalization to states never previously seen rather than overfitting to a single set of states and reward function. Focusing on generalization suggests a set of evaluation metrics in which at training time the agent learns from i.i.d. samples from the MDP distribution and at test time is evaluated on a new set of i.i.d. MDP samples drawn from the same distribution.  In this work, we formally describe our learning problem and evaluaiton metrics for it, present two approaches that address it, and review how existing work fits in with our problem formulation.


\section{Problem Definition}
Classic reinforcement learning (RL) problems are formulated as a trying to learn a (near-)optimal policy---a mapping from states to actions---in a Markov decision process (MDP). An MDP is defined by the tuple $(S, A, T, R)$, where $S$ is the state space, a set of states in which the agent can make decions; $A$ is the set of actions the agent can take; $T(s' | s, a)$ is the transition dynamics, which specifies the probability of transitioning to state $s'$ after taking action $a$ in state $s$; and $R(s, a, s')$ is the reward function, which specifies the reward received by the agent for taking action $a$ in state $s$ and then transitioning to state $s'$. Although there are other formulations, a policy $\pi : S \rightarrow A$ is typically considered optimal if following it maximizes the expected discounted future reward from each state $E \left[ \sum_{t=0}^{\infty} \gamma^t r_t \mid s, \pi \right]$, where $\gamma \in [0, 1]$ is a discount factor specifying how much the agent prefers immediate rewards over more distant rewards; $r_t$ is the reward received at time $t$; and $s$ is the state at time zero.

\begin{itemize}
\item overall problem definition
\item OO-MDP plug for representation
\item planning instantiation (true transition dynamics provided)
\item RL instantiation (true transition dynamics unknown)
\item evaluation metrics 
\end{itemize}

\section{Approaches}
\subsection{Goal-directed Action Priors (Affordances)}
\subsection{Meta-RL}

\section{Related Work}
\subsection{Model-based RL}
Learning a protable model (or model prior) over the distribution of MDPs is extremely powerful because it allows the agent plan out solutions to each novel situation without much needed exploration. Only tells part of the story, because it defers to a good planner being available, which may be difficult for complex domains. Therefore, model-learning transitions into the learning to plan problem.
\subsection{Bayesian RL}
Directly reasons over the distribution of MDPs and performs Bayes optimal learning. However, planning in this paradiagm is extremely complex and does not scale well. Like Model-based RL could substantially benefit from learning to plan.
\subsection{Transfer Learning}
Most RL transfer work focuses on transferring knowledge from a single or set of source tasks to a single target task. Here we are interested in generalized performance across a distirbution of tasks.
\subsection{Skill learning}
Most option learning work is limited to the same state space with different reward functions and there have been some negative results for learning with options. Current open problem with some success is learning protable options. State space abstraction+skill learning promising.


\section{Conclusion}

\bibliographystyle{plain}
\bibliography{groundhog}