The dominant approaches to reinforcement learning rely on a fixed
state-action space and reward function that the agent is trying to
maximize.  During training, the agent is repeatedly reset to a
predefined initial state or set of initial states.  For example, in
the classic RL Mountain Car domain, the agent starts at some point in
the valley, continues until it reaches the top of the valley and then
resets to somewhere else in the same valley. Learning in this regime
is akin to the learning problem faced by Bill Murray in the 1993 movie
{\em Groundhog Day} in which he repeatedly relives the same day, until
he discovers the optimal policy and escapes to the next day.  In a
more realistic formulation for an RL agent, every day is a new day
that may have similarities to the previous day, but the agent never
encounters the same state twice.  This formulation is a natural fit
for robotics problems in which a robot is placed in a room in which it
has never previously been, but has seen similar rooms with similar
objects in the past. We formalize this problem as optimizing a learning or planning
algorithm for a set of environments drawn from a distribution and present two sets of results
for learning under these settings. First, we present \emph{goal-based action priors} for learning how to accelerate planning in environments drawn from the distribution from a training set of environments drawn from the same distribution. Second, we present \emph{sample-optimized Rademacher complexity}, which is a formal mechanism for assessing the risk in choosing a learning algorithm tuned on a training set drawn from the distribution for use on the entire distribution.

